@misc{zhu_power_2025,
 abstract = {We formalize a new concept for LLMs, context-enhanced learning. It involves standard gradient-based learning on text except that the context is enhanced with additional data on which no auto-regressive gradients are computed. This setting is a gradient-based analog of usual in-context learning (ICL) and appears in some recent works. Using a multi-step reasoning task, we prove in a simplified setting that context-enhanced learning can be exponentially more sample-efficient than standard learning when the model is capable of ICL. At a mechanistic level, we find that the benefit of context-enhancement arises from a more accurate gradient learning signal. We also experimentally demonstrate that it appears hard to detect or recover learning materials that were used in the context during training. This may have implications for data security as well as copyright.},
 annote = {Comment: 76 pages, 17 figures; Pre-print},
 author = {Zhu, Xingyu and Panigrahi, Abhishek and Arora, Sanjeev},
 copyright = {All rights reserved},
 doi = {10.48550/arXiv.2503.01821},
 file = {Full Text PDF:files/930/Zhu et al. - 2025 - On the Power of Context-Enhanced Learning in LLMs.pdf:application/pdf;Snapshot:files/929/2503.html:text/html},
 keywords = {Computer Science - Machine Learning},
 month = {March},
 note = {arXiv:2503.01821 [cs]},
 publisher = {arXiv},
 title = {On the Power of Context-Enhanced Learning in LLMs},
 url = {http://arxiv.org/abs/2503.01821},
 urldate = {2025-03-20},
 year = {2025}
}
