@misc{zhu_understanding_2023,
 abstract = {Recently, researchers observed that gradient descent for deep neural networks operates in an ``edge-of-stability'' (EoS) regime: the sharpness (maximum eigenvalue of the Hessian) is often larger than stability threshold \$2/\eta\$ (where \$\eta\$ is the step size). Despite this, the loss oscillates and converges in the long run, and the sharpness at the end is just slightly below \$2/\eta\$. While many other well-understood nonconvex objectives such as matrix factorization or two-layer networks can also converge despite large sharpness, there is often a larger gap between sharpness of the endpoint and \$2/\eta\$. In this paper, we study EoS phenomenon by constructing a simple function that has the same behavior. We give rigorous analysis for its training dynamics in a large local region and explain why the final converging point has sharpness close to \$2/\eta\$. Globally we observe that the training dynamics for our example has an interesting bifurcating behavior, which was also observed in the training of neural nets.},
 annote = {Comment: 53 pages, 19 figures},
 author = {Zhu, Xingyu and Wang, Zixuan and Wang, Xiang and Zhou, Mo and Ge, Rong},
 copyright = {Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC-BY-NC-ND)},
 file = {arXiv.org Snapshot:files/15/2210.html:text/html;Full Text PDF:files/20/Zhu et al. - 2023 - Understanding Edge-of-Stability Training Dynamics .pdf:application/pdf},
 keywords = {Computer Science - Machine Learning, I.2.6, Statistics - Machine Learning, Mathematics - Optimization and Control},
 month = {February},
 note = {arXiv:2210.03294 [cs, math, stat]},
 publisher = {arXiv},
 title = {Understanding Edge-of-Stability Training Dynamics with a Minimalist Example},
 url = {http://arxiv.org/abs/2210.03294},
 urldate = {2023-09-20},
 year = {2023}
}
