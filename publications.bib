
@misc{wu_dissecting_2022,
	title = {Dissecting {Hessian}: {Understanding} {Common} {Structure} of {Hessian} in {Neural} {Networks}},
	copyright = {Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC-BY-NC-ND)},
	shorttitle = {Dissecting {Hessian}},
	url = {http://arxiv.org/abs/2010.04261},
	abstract = {Hessian captures important properties of the deep neural network loss landscape. Previous works have observed low rank structure in the Hessians of neural networks. In this paper, we propose a decoupling conjecture that decomposes the layer-wise Hessians of a network as the Kronecker product of two smaller matrices. We can analyze the properties of these smaller matrices and prove the structure of top eigenspace random 2-layer networks. The decoupling conjecture has several other interesting implications - top eigenspaces for different models have surprisingly high overlap, and top eigenvectors form low rank matrices when they are reshaped into the same shape as the corresponding weight matrix. All of these can be verified empirically for deeper networks. Finally, we use the structure of layer-wise Hessian to get better explicit generalization bounds for neural networks.},
	urldate = {2023-09-20},
	publisher = {arXiv},
	author = {Wu, Yikai and Zhu, Xingyu and Wu, Chenwei and Wang, Annie and Ge, Rong},
	month = oct,
	year = {2022},
	note = {arXiv:2010.04261 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, I.2.6, Statistics - Machine Learning},
	annote = {Comment: 72 pages, 31 figures. Main text: 10 pages, 7 figures. First two authors have equal contribution and are in alphabetical order},
	file = {arXiv.org Snapshot:files/11/2010.html:text/html;Full Text PDF:files/12/Wu et al. - 2022 - Dissecting Hessian Understanding Common Structure.pdf:application/pdf},
}

@misc{zhu_understanding_2023,
	title = {Understanding {Edge}-of-{Stability} {Training} {Dynamics} with a {Minimalist} {Example}},
	copyright = {Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC-BY-NC-ND)},
	url = {http://arxiv.org/abs/2210.03294},
	abstract = {Recently, researchers observed that gradient descent for deep neural networks operates in an ``edge-of-stability'' (EoS) regime: the sharpness (maximum eigenvalue of the Hessian) is often larger than stability threshold \$2/{\textbackslash}eta\$ (where \${\textbackslash}eta\$ is the step size). Despite this, the loss oscillates and converges in the long run, and the sharpness at the end is just slightly below \$2/{\textbackslash}eta\$. While many other well-understood nonconvex objectives such as matrix factorization or two-layer networks can also converge despite large sharpness, there is often a larger gap between sharpness of the endpoint and \$2/{\textbackslash}eta\$. In this paper, we study EoS phenomenon by constructing a simple function that has the same behavior. We give rigorous analysis for its training dynamics in a large local region and explain why the final converging point has sharpness close to \$2/{\textbackslash}eta\$. Globally we observe that the training dynamics for our example has an interesting bifurcating behavior, which was also observed in the training of neural nets.},
	urldate = {2023-09-20},
	publisher = {arXiv},
	author = {Zhu, Xingyu and Wang, Zixuan and Wang, Xiang and Zhou, Mo and Ge, Rong},
	month = feb,
	year = {2023},
	note = {arXiv:2210.03294 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, I.2.6, Statistics - Machine Learning, Mathematics - Optimization and Control},
	annote = {Comment: 53 pages, 19 figures},
	file = {arXiv.org Snapshot:files/15/2210.html:text/html;Full Text PDF:files/20/Zhu et al. - 2023 - Understanding Edge-of-Stability Training Dynamics .pdf:application/pdf},
}

@misc{shen_fairness_2023,
	title = {Fairness in the {Assignment} {Problem} with {Uncertain} {Priorities}},
	copyright = {Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC-BY-NC-ND)},
	url = {http://arxiv.org/abs/2301.13804},
	abstract = {In the assignment problem, a set of items must be allocated to unit-demand agents who express ordinal preferences (rankings) over the items. In the assignment problem with priorities, agents with higher priority are entitled to their preferred goods with respect to lower priority agents. A priority can be naturally represented as a ranking and an uncertain priority as a distribution over rankings. For example, this models the problem of assigning student applicants to university seats or job applicants to job openings when the admitting body is uncertain about the true priority over applicants. This uncertainty can express the possibility of bias in the generation of the priority ranking. We believe we are the first to explicitly formulate and study the assignment problem with uncertain priorities. We introduce two natural notions of fairness in this problem: stochastic envy-freeness (SEF) and likelihood envy-freeness (LEF). We show that SEF and LEF are incompatible and that LEF is incompatible with ordinal efficiency. We describe two algorithms, Cycle Elimination (CE) and Unit-Time Eating (UTE) that satisfy ordinal efficiency (a form of ex-ante Pareto optimality) and SEF; the well known random serial dictatorship algorithm satisfies LEF and the weaker efficiency guarantee of ex-post Pareto optimality. We also show that CE satisfies a relaxation of LEF that we term 1-LEF which applies only to certain comparisons of priority, while UTE satisfies a version of proportional allocations with ranks. We conclude by demonstrating how a mediator can model a problem of school admission in the face of bias as an assignment problem with uncertain priority.},
	urldate = {2023-09-20},
	publisher = {arXiv},
	author = {Shen, Zeyu and Wang, Zhiyi and Zhu, Xingyu and Fain, Brandon and Munagala, Kamesh},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13804 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory},
	annote = {Comment: Accepted to AAMAS 2023},
	file = {arXiv.org Snapshot:files/18/2301.html:text/html;Full Text PDF:files/19/Shen et al. - 2023 - Fairness in the Assignment Problem with Uncertain .pdf:application/pdf},
}

@misc{zhu_power_2025,
	title = {On the {Power} of {Context}-{Enhanced} {Learning} in {LLMs}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/2503.01821},
	doi = {10.48550/arXiv.2503.01821},
	abstract = {We formalize a new concept for LLMs, context-enhanced learning. It involves standard gradient-based learning on text except that the context is enhanced with additional data on which no auto-regressive gradients are computed. This setting is a gradient-based analog of usual in-context learning (ICL) and appears in some recent works. Using a multi-step reasoning task, we prove in a simplified setting that context-enhanced learning can be exponentially more sample-efficient than standard learning when the model is capable of ICL. At a mechanistic level, we find that the benefit of context-enhancement arises from a more accurate gradient learning signal. We also experimentally demonstrate that it appears hard to detect or recover learning materials that were used in the context during training. This may have implications for data security as well as copyright.},
	urldate = {2025-03-20},
	publisher = {arXiv},
	author = {Zhu, Xingyu and Panigrahi, Abhishek and Arora, Sanjeev},
	month = mar,
	year = {2025},
	note = {arXiv:2503.01821 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: 76 pages, 17 figures; Pre-print},
	file = {Full Text PDF:files/930/Zhu et al. - 2025 - On the Power of Context-Enhanced Learning in LLMs.pdf:application/pdf;Snapshot:files/929/2503.html:text/html},
}
